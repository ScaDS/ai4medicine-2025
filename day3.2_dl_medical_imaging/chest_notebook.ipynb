{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbca705",
   "metadata": {},
   "source": [
    "# Chest X-Ray abnormality classification\n",
    "\n",
    "In the following, we are training a model that detects the following abnormalities from chest X-Rays: Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion, Emphysema, Fibrosis, Hernia, Infiltration, Mass, Nodule, Pleural_Thickening, Pneumonia, Pneumothorax.\n",
    "\n",
    "For this, we will use the NIH Chest X-ray Dataset of 14 Common Thorax Disease Categories [1,2]:\n",
    "\n",
    "[1] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, Ronald Summers,ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised\n",
    "\n",
    "[2] Classification and Localization of Common Thorax Diseases, IEEE CVPR, pp. 3462-3471, 2017 Hoo-chang Shin, Kirk Roberts, Le Lu, Dina Demner-Fushman, Jianhua Yao, Ronald M. Summers, Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation, IEEE CVPR, pp. 2497-2506, 2016\n",
    "\n",
    "For training, we need pairs of images and their associated findings, indicating if each abnormality is present or not.\n",
    "\n",
    "The images are stored in dataset_path. The findings are stored in the table.\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d1968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd068f05",
   "metadata": {},
   "source": [
    "## Data Exploration 1\n",
    "Now, let's visualize the table that contains the abnormality findings associated to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f915b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"/data/horse/ws/lazi257c-come2data_workshop/radiology_session/chest_x_ray\")\n",
    "table_path = dataset_path / \"Data_Entry_2017.csv\"\n",
    "\n",
    "image_paths = list((dataset_path / 'preprocessed').rglob(\"*.png\"))\n",
    "\n",
    "path_dict = {p.name: p for p in image_paths}\n",
    "\n",
    "table_df = pd.read_csv(table_path)\n",
    "\n",
    "table_df[\"image_path\"] = table_df[\"Image Index\"].map(path_dict)\n",
    "\n",
    "# Split labels into lists\n",
    "table_df[\"Finding Labels\"] = table_df[\"Finding Labels\"].str.split(\"|\")\n",
    "\n",
    "# Collect all unique disease labels (excluding \"No Finding\")\n",
    "all_labels = sorted(\n",
    "    set(l for sublist in table_df[\"Finding Labels\"] for l in sublist if l != \"No Finding\")\n",
    ")\n",
    "\n",
    "# Create one column per disease with 0/1\n",
    "for label in all_labels:\n",
    "    table_df[label] = table_df[\"Finding Labels\"].apply(lambda x: 1 if label in x else 0)\n",
    "\n",
    "cols_to_show = [\n",
    "    \"Image Index\", \"Patient ID\",\n",
    "    \"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Effusion\",\n",
    "    \"Emphysema\", \"Fibrosis\", \"Hernia\", \"Infiltration\", \"Mass\", \"Nodule\",\n",
    "    \"Pleural_Thickening\", \"Pneumonia\", \"Pneumothorax\"\n",
    "]\n",
    "print(\"This is what the table looks like\")\n",
    "table_df[cols_to_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42591379",
   "metadata": {},
   "source": [
    "## Data Exploration 2\n",
    "Let's show how many patients and images per abnormality class are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_cols = [\n",
    "    \"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Effusion\",\n",
    "    \"Emphysema\", \"Fibrosis\", \"Hernia\", \"Infiltration\", \"Mass\", \"Nodule\",\n",
    "    \"Pleural_Thickening\", \"Pneumonia\", \"Pneumothorax\"\n",
    "]\n",
    "\n",
    "# Collapse per patient (max across their images → whether patient ever had that finding)\n",
    "per_patient = table_df.groupby(\"Patient ID\")[disease_cols].max()\n",
    "\n",
    "# Count patients and images per disease\n",
    "patient_counts = per_patient.sum().astype(int)\n",
    "image_counts   = table_df[disease_cols].sum().astype(int)\n",
    "\n",
    "# Combine into a DataFrame for sorting\n",
    "summary_df = pd.DataFrame({\n",
    "    \"patients\": patient_counts,\n",
    "    \"images\": image_counts\n",
    "}).sort_values(\"patients\", ascending=False)\n",
    "\n",
    "# Pretty print\n",
    "for disease, row in summary_df.iterrows():\n",
    "    print(f\"Number of patients (images) with {disease:<20}: {row['patients']} ({row['images']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a2747",
   "metadata": {},
   "source": [
    "## Data Exploration 3\n",
    "Let's show 5 random images and their associated finding(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 3 random samples that have valid images\n",
    "samples = table_df.sample(5, random_state=222)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, (_, row) in enumerate(samples.iterrows()):\n",
    "    img = Image.open(dataset_path / \"preprocessed\" / row[\"Image Index\"]).convert(\"L\")\n",
    "    findings = [d for d in disease_cols if row[d] == 1]\n",
    "    title = \", \".join(findings) if findings else \"No Finding\"\n",
    "\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025496e7",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "Now, let's split the data into a training set, validation set and test set. Each patient is either in the training set, validation set or test set to avoid data leakage.\n",
    "\n",
    "- Training set: images used to teach the model to recognize patterns.\n",
    "\n",
    "- Validation set: images checked during training to see how well the model is learning.\n",
    "\n",
    "- Test set: images kept aside until the end to measure how well the model performs on unseen cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# ==== SPLIT: patient-level (80/20 train/val) ====\n",
    "rng = np.random.default_rng(222)\n",
    "patients = np.array(table_df[\"Image Index\"].unique())\n",
    "rng.shuffle(patients)\n",
    "\n",
    "n = len(patients)\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "# test = rest\n",
    "\n",
    "train_patients = set(patients[:n_train])\n",
    "val_patients   = set(patients[n_train:n_train + n_val])\n",
    "test_patients  = set(patients[n_train + n_val:])\n",
    "\n",
    "train_df = table_df[table_df[\"Image Index\"].isin(train_patients)].reset_index(drop=True)\n",
    "val_df   = table_df[table_df[\"Image Index\"].isin(val_patients)].reset_index(drop=True)\n",
    "test_df  = table_df[table_df[\"Image Index\"].isin(test_patients)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Patients: total={n} | train={len(train_patients)} | val={len(val_patients)} | test={len(test_patients)}\")\n",
    "print(f\"Images: train={len(train_df)} | val={len(val_df)} | test={len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca829ad6",
   "metadata": {},
   "source": [
    "## Dataset and Dataloaders\n",
    "We define a custom `CXRDataset` for loading chest X-ray images and their multi-label annotations.  \n",
    "- Images are converted to grayscale and then expanded to 3 channels for ResNet compatibility.  \n",
    "- Transforms normalize images to ImageNet statistics (no augmentation).  \n",
    "- DataLoaders are created for train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== DATASET ====\n",
    "class CXRDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, all_labels, transform=None):\n",
    "        self.df = df\n",
    "        self.all_labels = all_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"image_path\"]).convert(\"L\")  # grayscale\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        # Labels as float tensor (multi-label)\n",
    "        y = torch.tensor(row[self.all_labels].values.astype(np.float32))\n",
    "        return img, y\n",
    "\n",
    "# Transforms: no augmentation; just make 3-channel + normalize to ImageNet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # ResNet expects 3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_ds = CXRDataset(train_df, all_labels, transform=transform)\n",
    "val_ds   = CXRDataset(val_df,   all_labels, transform=transform)\n",
    "test_ds   = CXRDataset(test_df,   all_labels, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader   = DataLoader(test_ds,   batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995823bf",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "- Loads a ResNet-18 model (a well-known image classifier).\n",
    "\n",
    "- Replaces its last layer so it can predict our 14 abnormality findings.\n",
    "\n",
    "- Moves the model to the GPU (if available) for faster training.\n",
    "\n",
    "- Defines the loss function (how the model measures its discrepancy between predictions and true labels) and the optimizer (how it learns from the true labels).\n",
    "\n",
    "- Finally, shows a layer-by-layer summary of the model architecture and the number of parameters it will train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80716f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "# ==== MODEL ====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, len(all_labels))  # multi-label logits\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "warmup_steps = 200\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: min(1.0, float(step + 1) / warmup_steps),\n",
    ")\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb5e32",
   "metadata": {},
   "source": [
    "## Model Evaluation & Training  \n",
    "This cell handles two key parts:  \n",
    "\n",
    "**Evaluation**  \n",
    "- Tests the model on validation data.  \n",
    "- Reports how well it separates positive vs. negative cases using the AUROC metric.  \n",
    "\n",
    "**Training**  \n",
    "- Trains step by step and checks progress every few rounds.  \n",
    "- Stops early if performance no longer improves (to avoid overfitting).  \n",
    "- Saves the best version of the model.  \n",
    "- Records results in a log file for later plotting.  \n",
    "\n",
    "AUROC is a metric commonly used for evaluating classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea81424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== METRICS ====\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_targets = []\n",
    "    running_val_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for imgs, ys in loader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        ys   = ys.to(device, non_blocking=True)\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, ys)\n",
    "        running_val_loss += loss.item()\n",
    "        n_batches += 1\n",
    "        all_logits.append(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "        all_targets.append(ys.detach().cpu().numpy())\n",
    "    val_loss = running_val_loss / max(1, n_batches)\n",
    "    y_prob = np.concatenate(all_logits, axis=0)\n",
    "    y_true = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Per-class AUROC with safe handling for degenerate classes\n",
    "    aurocs = []\n",
    "    for ci in range(y_true.shape[1]):\n",
    "        y_c = y_true[:, ci]\n",
    "        p_c = y_prob[:, ci]\n",
    "        # skip if only one class present\n",
    "        if len(np.unique(y_c)) < 2:\n",
    "            aurocs.append(np.nan)\n",
    "            continue\n",
    "        try:\n",
    "            auc = roc_auc_score(y_c, p_c)\n",
    "        except Exception:\n",
    "            auc = np.nan\n",
    "        aurocs.append(auc)\n",
    "    mean_auroc = np.nanmean(aurocs)  # mean over classes that have both labels\n",
    "    return val_loss, mean_auroc, aurocs\n",
    "\n",
    "# ==== TRAIN LOOP (step-based) ====\n",
    "max_steps = 5000\n",
    "validate_every = 100\n",
    "patience = 5  # early stopping patience on val AUROC\n",
    "best_val_auroc = -math.inf\n",
    "since_improve = 0\n",
    "\n",
    "history = []  # list of dicts: step, train_loss, val_loss, val_auroc\n",
    "global_step = 0\n",
    "\n",
    "model.train()\n",
    "running_train_loss = 0.0\n",
    "batches_since_log = 0\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "while global_step < max_steps:\n",
    "    try:\n",
    "        imgs, ys = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        imgs, ys = next(train_iter)\n",
    "\n",
    "    imgs = imgs.to(device, non_blocking=True)\n",
    "    ys   = ys.to(device, non_blocking=True)\n",
    "\n",
    "    logits = model(imgs)\n",
    "    loss = criterion(logits, ys)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    running_train_loss += loss.item()\n",
    "    batches_since_log += 1\n",
    "    global_step += 1\n",
    "\n",
    "    # Validation & logging\n",
    "    if global_step % validate_every == 0 or global_step == max_steps:\n",
    "        train_loss = running_train_loss / max(1, batches_since_log)\n",
    "        running_train_loss = 0.0\n",
    "        batches_since_log = 0\n",
    "\n",
    "        val_loss, val_mean_auroc, _ = evaluate(model, val_loader)\n",
    "\n",
    "        history.append({\n",
    "            \"step\": global_step,\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"val_mean_auroc\": float(val_mean_auroc),\n",
    "        })\n",
    "        print(f\"[step {global_step:5d}] train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_mean_AUROC={val_mean_auroc:.4f}\")\n",
    "\n",
    "        # Early stopping on val AUROC\n",
    "        if val_mean_auroc > best_val_auroc + 1e-6:\n",
    "            best_val_auroc = val_mean_auroc\n",
    "            since_improve = 0\n",
    "            # save best\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"all_labels\": all_labels,\n",
    "                \"val_mean_auroc\": best_val_auroc,\n",
    "                \"step\": global_step,\n",
    "            }, \"best_resnet18_multilabel.pt\")\n",
    "        else:\n",
    "            since_improve += 1\n",
    "            if since_improve >= patience:\n",
    "                print(f\"Early stopping at step {global_step} (no improvement in {patience} validations).\")\n",
    "                break\n",
    "\n",
    "# Save training log for plotting later\n",
    "log_df = pd.DataFrame(history)\n",
    "log_csv_path = \"training_log.csv\"\n",
    "log_df.to_csv(log_csv_path, index=False)\n",
    "print(f\"Saved log to {log_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a5419",
   "metadata": {},
   "source": [
    "## Training Visualization\n",
    "The figure shows how the model’s training loss, validation loss, and validation AUROC change over time.\n",
    "\n",
    "**Overfitting**:\n",
    "\n",
    "- Training loss: keeps decreasing.\n",
    "\n",
    "- Validation loss: decreases at first, then starts increasing.\n",
    "\n",
    "- Validation AUROC (or accuracy/other metric): peaks and then declines.\n",
    "\n",
    "- Interpretation: The model is learning patterns specific to the training set (memorization), losing the ability to generalize.\n",
    "\n",
    "**Underfitting**\n",
    "\n",
    "- Training loss: remains high or decreases slowly.\n",
    "\n",
    "- Validation loss: also high, often decreasing in parallel with training loss (not yet separated).\n",
    "\n",
    "- Validation AUROC/metric: remains low, may still be rising, has not plateaued.\n",
    "\n",
    "- Interpretation: The model is too simple (or hasn’t trained enough) to capture meaningful patterns.\n",
    "\n",
    "**Good fit**\n",
    "\n",
    "- Validation loss: minimum reached.\n",
    "\n",
    "- Validation AUROC/metric: maximum reached (green dot).\n",
    "\n",
    "- Interpretation: The model can generalize to unseen data.\n",
    "\n",
    "Did the model over/underfit or was the training successful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeaf10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_csv(log_csv_path)\n",
    "\n",
    "\n",
    "# --- Plot Loss and AUROC in two columns ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), dpi=150)\n",
    "\n",
    "# --- Left plot: Training & Validation Loss ---\n",
    "axes[0].plot(log_df[\"step\"], log_df[\"train_loss\"], label=\"Training loss\")\n",
    "axes[0].plot(log_df[\"step\"], log_df[\"val_loss\"], label=\"Validation loss\")\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training & Validation Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "# --- Right plot: Validation AUROC ---\n",
    "axes[1].plot(log_df[\"step\"], log_df[\"val_mean_auroc\"], label=\"Validation AUROC\", color=\"C2\")\n",
    "best_idx = log_df[\"val_mean_auroc\"].idxmax()\n",
    "best_step = log_df.loc[best_idx, \"step\"]\n",
    "best_auroc = log_df.loc[best_idx, \"val_mean_auroc\"]\n",
    "axes[1].scatter(best_step, best_auroc, color=\"C2\", zorder=5)\n",
    "axes[1].annotate(f\"Best AUROC = {best_auroc:.3f}\\n(step {best_step})\",\n",
    "                 (best_step, best_auroc),\n",
    "                 textcoords=\"offset points\", xytext=(-20, -30),\n",
    "                 ha=\"left\", color=\"C2\")\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"AUROC\")\n",
    "axes[1].set_title(\"Validation AUROC\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_and_auroc.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved: 'loss_and_auroc.png'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd248d4",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Set\n",
    "Now, we evaluate the trained model on the independent test set. This is the only way to know how well the model performs on unseen data. We also visualize the ROC curves, which show the trade-off between sensitivity and specificity. Sensitivity reflects how well the model finding abnormal cases, while specificity reflects how well it rules out healthy cases. These two measures are always in balance: improving sensitivity usually lowers specificity, and vice versa. The red dot marks a decision boundary where both sensitivity and specificity are reasonably high, representing a good compromise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082eff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==== FINAL TEST EVAL & PLOTTING ====\n",
    "import json\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load best checkpoint (if available)\n",
    "ckpt_path = \"best_resnet18_multilabel.pt\"\n",
    "ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "print(f\"Loaded best checkpoint from step {ckpt.get('step', 'N/A')} with val_mean_auroc={ckpt.get('val_mean_auroc', None)}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_probs(model, loader):\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "    targets_list = []\n",
    "    for imgs, ys in loader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        ys   = ys.to(device, non_blocking=True)\n",
    "        logits = model(imgs)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        probs_list.append(probs.cpu().numpy())\n",
    "        targets_list.append(ys.cpu().numpy())\n",
    "    return np.concatenate(probs_list, axis=0), np.concatenate(targets_list, axis=0)\n",
    "\n",
    "y_prob_test, y_true_test = predict_probs(model, test_loader)\n",
    "print(f\"Test shapes: probs={y_prob_test.shape}, targets={y_true_test.shape}\")\n",
    "\n",
    "per_class_results = []\n",
    "mean_aurocs = []\n",
    "\n",
    "# --- Plot all ROC curves in one figure (2 rows × 7 cols) ---\n",
    "n_classes = len(all_labels)\n",
    "fig, axes = plt.subplots(2, 7, figsize=(20, 8), dpi=150)\n",
    "\n",
    "for ci, cls in enumerate(all_labels):\n",
    "    row, col = divmod(ci, 7)\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    y_c = y_true_test[:, ci]\n",
    "    p_c = y_prob_test[:, ci]\n",
    "\n",
    "    # Skip degenerate classes\n",
    "    if len(np.unique(y_c)) < 2:\n",
    "        ax.set_title(f\"{cls}\\n(skipped)\")\n",
    "        ax.axis(\"off\")\n",
    "        continue\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_c, p_c)\n",
    "    class_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Youden's J statistic\n",
    "    J = tpr - fpr\n",
    "    j_idx = int(np.argmax(J))\n",
    "    thr_opt = thresholds[j_idx]\n",
    "    tpr_opt = tpr[j_idx]\n",
    "    fpr_opt = fpr[j_idx]\n",
    "\n",
    "    # Plot ROC and Youden point\n",
    "    ax.plot(fpr, tpr, label=f\"AUC = {class_auc:.3f}\")\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    ax.scatter([fpr_opt], [tpr_opt], s=30, marker=\"o\", color=\"red\")  # Youden point\n",
    "    ax.set_title(cls, fontsize=10)\n",
    "    ax.set_xlabel(\"1 – Specificity\")\n",
    "    ax.set_ylabel(\"Sensitivity\")\n",
    "    ax.legend(loc=\"lower right\", fontsize=8)\n",
    "\n",
    "    per_class_results.append({\n",
    "        \"class\": cls,\n",
    "        \"auroc\": float(class_auc),\n",
    "        \"opt_threshold\": float(thr_opt),\n",
    "        \"opt_tpr\": float(tpr_opt),\n",
    "        \"opt_fpr\": float(fpr_opt),\n",
    "    })\n",
    "    mean_aurocs.append(float(class_auc))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "out_path = \"roc_all_classes.png\"\n",
    "plt.savefig(out_path)\n",
    "plt.close()\n",
    "print(f\"Saved combined ROC plot: {out_path}\")\n",
    "\n",
    "# Mean AUROC across classes that had both labels\n",
    "test_mean_auroc = float(np.nanmean(mean_aurocs)) if len(mean_aurocs) else float(\"nan\")\n",
    "print(f\"TEST mean AUROC (across valid classes): {test_mean_auroc:.4f}\")\n",
    "summary_csv = \"test_results_summary.csv\"\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "print(f\"Saved per-class summary: {summary_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91f734",
   "metadata": {},
   "source": [
    "## Explainability\n",
    "Now, we will apply an explainability method that should tell us why the model predicted the way it did. A simple and commonly used method is creating heatmaps using Grad-CAMs. Put simply, Grad-CAM highlights the areas in the X-ray image that contributed most to the model’s decision, showing us whether the model is focusing on clinically relevant regions. \n",
    "\n",
    "We will visualize the heatmaps from five random pneumothorax cases. Do the heatmaps make any sense, i.e. do they highlight meaningful areas? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== GRAD-CAM (Captum) — 5 random Pneumothorax test images (categorical titles via ROC threshold) ====\n",
    "from captum.attr import LayerGradCam, LayerAttribution\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "assert \"Pneumothorax\" in all_labels, \"Pneumothorax not found in all_labels!\"\n",
    "cls_idx = all_labels.index(\"Pneumothorax\")\n",
    "\n",
    "# Build a lookup for optimal thresholds computed above (Youden's J)\n",
    "thr_lookup = {r[\"class\"]: r[\"opt_threshold\"] for r in per_class_results if not np.isnan(r[\"opt_threshold\"])}\n",
    "thr_pneu = thr_lookup[\"Pneumothorax\"]\n",
    "# Select only true Pneumothorax cases from the test set\n",
    "pneu_cases = test_df[test_df[\"Pneumothorax\"] == 1]\n",
    "n_show = min(5, len(pneu_cases))\n",
    "samples = pneu_cases.sample(n_show, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Grad-CAM setup on the last ResNet conv block\n",
    "target_layer = model.layer4[-1]\n",
    "gradcam = LayerGradCam(model, target_layer)\n",
    "model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(1, n_show, figsize=(3.6*n_show, 4), dpi=150)\n",
    "\n",
    "# Handle case when n_show == 1 (axes not iterable)\n",
    "if n_show == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, row in samples.iterrows():\n",
    "    img_path = row[\"image_path\"]\n",
    "    pil_img = Image.open(img_path).convert(\"L\")\n",
    "    rgb_for_overlay = pil_img.convert(\"RGB\")\n",
    "    x = transform(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        logits = model(x)\n",
    "        prob = torch.sigmoid(logits)[0, cls_idx].item()\n",
    "        attrs = gradcam.attribute(inputs=x, target=cls_idx)  # [1, C, h, w]\n",
    "        cam = LayerAttribution.interpolate(attrs, x.shape[2:]).detach().cpu().numpy()\n",
    "        cam = cam.mean(axis=1)[0]  # [H, W]\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "\n",
    "    # Binarize prediction using ROC-derived threshold\n",
    "    pred_label = int(prob >= thr_pneu)\n",
    "    true_label = int(row[\"Pneumothorax\"])\n",
    "    pred_text = \"Positive\" if pred_label == 1 else \"Negative\"\n",
    "    label_text = \"Positive\" if true_label == 1 else \"Negative\"\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.imshow(rgb_for_overlay)\n",
    "    im = ax.imshow(cam, cmap=\"jet\", alpha=0.15)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Pneumothorax:\\nModel: {pred_text}\\nGround Truth: {label_text}\", fontsize=9)\n",
    "\n",
    "# Add a single colorbar for the heatmaps\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.015, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "cbar.set_label(\"Grad-CAM importance\\nLow → High\", fontsize=9)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "out_path = \"gradcam_pneumothorax_5_examples.png\"\n",
    "plt.show()\n",
    "plt.savefig(out_path, bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.close()\n",
    "print(f\"Saved 5-panel Grad-CAM figure with categorical predictions: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32200f",
   "metadata": {},
   "source": [
    "## Optional: Training a Larger Model   \n",
    "\n",
    "If you still have time, define a larger model, e.g. a resnet50, and train again. You can use the code from above.\n",
    "Observe, if the performance of the larger model changes. Do you know why?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca744b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "# Now train the model again and evaluate it afterwards. You can copy the code from above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
